

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Basic Concepts &mdash; DI-engine 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="RL Algorithm Cheat Sheet" href="../hands_on/index.html" />
    <link rel="prev" title="Introduction to RL" href="index.html" />
    <link href="../_static/css/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> DI-engine
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key_concept/index.html">Key Concept</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Introduction to RL</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Basic Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#markov-decision-process-mdp">Markov Decision Process/MDP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#state-spaces">State Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="#action-spaces">Action Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="#policy">Policy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#trajectory">Trajectory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#return-and-reward">Return and reward</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rl-optimization-problem">RL optimization problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#value-functions">Value functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#policy-gradients">Policy Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="#actor-critic">Actor Critic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-based-rl">Model-based RL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#q-a">Q&amp;A</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../hands_on/index.html">RL Algorithm Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_tutorial/index.html">RL Environments Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed/index.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../best_practice/index.html">Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/index.html">API Doc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature/index.html">Feature</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_dev/index.html">Tutorial-Developer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/index.html">Architecture Design</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DI-engine</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Introduction to RL</a> &raquo;</li>
        
      <li>Basic Concepts</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/intro_rl/basic.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="basic-concepts">
<h1>Basic Concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this headline">¶</a></h1>
<p>Reinforcement learning (RL) has been used to solve the interaction problem between agents and environments. The interaction process can be simply described as follows: An agent receives observation from the environment, and acts accordingly. The environment will change due to the action taken by the agent and send a reward to the agent. This process runs repeatedly and the goal of the agent is to maximise the cumulative reward (sum of (discounted) rewards) received. The objective of reinforcement learning is to enable agents to learning strategies which we call ‘policy’ and maximise the cumulative reward.</p>
<p>To have a basic understanding for reinforcement learning, we explain the following basic concepts：</p>
<ul class="simple">
<li><p>Markov Decision Processes</p></li>
<li><p>State and action spaces</p></li>
<li><p>Policy</p></li>
<li><p>Trajectory</p></li>
<li><p>Return and reward</p></li>
</ul>
<p>To understand reinforcement learning better, we further explain the following concepts:</p>
<ul class="simple">
<li><p>RL optimization problem</p></li>
<li><p>Value function</p></li>
<li><p>Policy gradients</p></li>
<li><p>Actor Critic</p></li>
<li><p>Model-based RL</p></li>
</ul>
<p>In the end, we will list and answer some common questions raised in the domain of reinforcement learning for reference.</p>
<div class="section" id="markov-decision-process-mdp">
<h2>Markov Decision Process/MDP<a class="headerlink" href="#markov-decision-process-mdp" title="Permalink to this headline">¶</a></h2>
<p><strong>Markov Decision Process (MDP)</strong> is the ideal mathematical model of reinforcement learning and the commonest one.</p>
<ul class="simple">
<li><p>Markov property：State <img class="math" src="../_images/math/4aabbc49ec51d4fbf17ec43338f4592bda61837d.svg" alt="s_t"/> is Markov, iff <img class="math" src="../_images/math/59917b4a85b8ac4c38540e4ba50e2fbe1f9694bd.svg" alt="P[s_{t+1}|s_t] = P[s_{t+1}|s_1, ..., s_t]"/> .</p></li>
<li><p>A Markov process is a memoryless random process, i.e., a sequence of random states S_1,　S_2… with the Markov process.</p></li>
<li><p>Markov process is a binary tuple <img class="math" src="../_images/math/fc6f9a16e236449fbd45739d491cbf98f1c68f6e.svg" alt="(S, P)"/> , satisfying: <img class="math" src="../_images/math/bb8cc39c82f0fa9cae75f7d6e20647c99eb5d783.svg" alt="S"/> a finite set of states, <img class="math" src="../_images/math/dfb6e4180d45e8c514cc6f128645a36de19230d0.svg" alt="P"/> is a probability transition matrix. There are no rewards or actions in a Markov process. A Markov process that takes actions and rewards into account is called a MDP.</p></li>
<li><p>MDP is a tuple <img class="math" src="../_images/math/d351ed0a9f50be00128ac370bf92e75781712f90.svg" alt="(S, A, P, R, \gamma)"/>， <img class="math" src="../_images/math/bb8cc39c82f0fa9cae75f7d6e20647c99eb5d783.svg" alt="S"/> is a finite set of states, <img class="math" src="../_images/math/bb2fa1da2a0d41cf7aa7ae01a9d32b98affa34b9.svg" alt="A"/> is a finite set of actions， <img class="math" src="../_images/math/dfb6e4180d45e8c514cc6f128645a36de19230d0.svg" alt="P"/> is a state transition probability matrix, <img class="math" src="../_images/math/67cd39021e4f83e4e1fd43e6cd911774ba6eb877.svg" alt="R"/> is a reward function， <img class="math" src="../_images/math/1f7c32cd87f6900b6edb43745cc6eeb23eade7b4.svg" alt="\gamma"/> is a discount factor used to calculate the accumulated rewards. Unlike the Markov process, the state transfer probability of the Markov decision process is <img class="math" src="../_images/math/0e83b04c66719212b0a61a2f2596993869353196.svg" alt="P(s_{t+1}|s_t, a_t)"/> . <img class="math" src="../_images/math/0e83b04c66719212b0a61a2f2596993869353196.svg" alt="P(s_{t+1}|s_t, a_t)"/> .</p></li>
<li><p>The goal of reinforcement learning is to seek an optimal policy based on a MDP.  The so-called policy <img class="math" src="../_images/math/64fff4391278558d9305d277c3f7e13b094677a2.svg" alt="\pi(a|s)"/> refers to the mapping of states to actions. In reinforcement learning, we only discuss Markov decision processes with finite state space.</p></li>
</ul>
<p>Common methods for solving MDP problems:</p>
<ol class="arabic">
<li><p><strong>Dynamic programming (DP)</strong> is an optimization method that can compute the optimal policy given an MDP. However, for reinforcement learning problems, traditional DP is of limited use and prone to dimensional catastrophe problems.</p>
<p>DP has the following characteristics:</p>
<ul class="simple">
<li><p>Update is based on currently existing estimates: the value estimate of a current state is updated with the value estimate of each of its subsequent states</p></li>
<li><p>Asymptotic convergence</p></li>
<li><p>Advantages: reduced variance and faster learning</p></li>
<li><p>Disadvantages: bias dependent on the quality of the function approximation</p></li>
</ul>
</li>
<li><p><strong>Monte Carlo methods (MC)</strong>  are based directly on the definition of the optimal value function and provide an unbiased estimate of the optimal value function by sampling. MC replaces the actual expected return with the sample return and solves the optimal strategy empirically only.</p>
<p>MC do not need an environmental model. Data simulation and sampling models can be applied to MC and MC can only evaluate a certain state of interest. In comparison with DP, MC performs better when the Markov property does not hold.</p>
</li>
<li><p><strong>Temperal-Differnece learning (TD)</strong>, TD error: <img class="math" src="../_images/math/a631677fbca91d568d1864efbdba5071dba0eca9.svg" alt="\delta_{t} = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)"/></p>
<p>Comparison between TD and MC: The target for MC is <img class="math" src="../_images/math/fb37bd0a717ca454ea94af8da3262d4c07a917e1.svg" alt="G_t"/>, namely, the real return from time t onwards. However, the target for TD (single step TD, TD(0)) is  <img class="math" src="../_images/math/bfc98a2e2587e9e6d72faca04060b2853623c548.svg" alt="R_{t+1} + \gamma V(S_{t+1})"/> .</p>
</li>
</ol>
</div>
<div class="section" id="state-spaces">
<h2>State Spaces<a class="headerlink" href="#state-spaces" title="Permalink to this headline">¶</a></h2>
<p>State <img class="math" src="../_images/math/6ca1d8558037c00a979b111265e9fd2de5069055.svg" alt="s"/> is a global description of the environment，observation <img class="math" src="../_images/math/2b436cd73add7d7c76d4a2c7ea4918320a69faa1.svg" alt="o"/> is a partial description of the environment. States and observations from an environment can be represented by real vectors, matrix or tensors. For example, RGB pictures are used in Atari games to represent information about the game environment, and vectors are used in MuJoCo control tasks to represent the state of an intelligent body.</p>
<p>When an agent can receive all information of states in an environment <img class="math" src="../_images/math/6ca1d8558037c00a979b111265e9fd2de5069055.svg" alt="s"/> ，we call its learning process fully observable. When an agent can only receive partial information of states in an environment <img class="math" src="../_images/math/2b436cd73add7d7c76d4a2c7ea4918320a69faa1.svg" alt="o"/>，we call its learning process partially observable，namely, partially observable Markov decision processes (POMDP) <img class="math" src="../_images/math/01f6c4b1e847508b12d1bc58e82efe0ecbe1626b.svg" alt="(O, A, P, R, \gamma)"/>.</p>
</div>
<div class="section" id="action-spaces">
<h2>Action Spaces<a class="headerlink" href="#action-spaces" title="Permalink to this headline">¶</a></h2>
<p>Different environments allow different action space. The set of all valid actions <img class="math" src="../_images/math/becb5d56210daf0b2a97f9aa1207dc03838b9fbd.svg" alt="a"/> in an environment is generally referred to as the Action Space. The action space can be classified into a discrete action space or a continuous action space.</p>
<p>For example, in Atari games and SMAC games, the action spaces are both discrete and only a limited number of actions can be selected from each space. However, in some robot continuous control tasks such as MuJoCo, the action space is continuous and generally belong to a real-valued vector interval.</p>
</div>
<div class="section" id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Permalink to this headline">¶</a></h2>
<p><strong>Policy</strong> determines actions an agent takes when facing different states. If a policy is deterministic, it is usually denoted by <img class="math" src="../_images/math/c0fb6749af3f3266661a04bbb8ef6f5c6f240e58.svg" alt="a_t = \mu(s_t)"/> .
when a policy is stochastic，it is usually denoted by <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">a_t ~ \pi(·｜s_t')</span>)</p>
<p>latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.20 (TeX Live 2019/Debian) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2020-02-02&gt; patch level 2
L3 programming layer &lt;2020-02-14&gt;
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2019/12/20 v1.4l Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size12.clo))
(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))
(/usr/share/texlive/texmf-dist/tex/latex/amscls/amsthm.sty)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))
(/usr/share/texlive/texmf-dist/tex/latex/anyfontsize/anyfontsize.sty)
(/usr/share/texlive/texmf-dist/tex/latex/tools/bm.sty)
(/usr/share/texlive/texmf-dist/tex/latex/algorithms/algorithm.sty
(/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)
(/usr/share/texlive/texmf-dist/tex/latex/base/ifthen.sty))
(/usr/share/texlive/texmf-dist/tex/latex/algorithms/algorithmic.sty
(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty))
(/usr/share/texlive/texmf-dist/tex/latex/cancel/cancel.sty)
(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty
(/usr/share/texlive/texmf-dist/tex/generic/iftex/ifvtex.sty
(/usr/share/texlive/texmf-dist/tex/generic/iftex/iftex.sty)))

Package geometry Warning: `bmargin' results in NEGATIVE (-144.54pt).
    `height' or `tmargin' should be shortened in length.

(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-dvips.def)
(./math.aux)
*geometry* driver: auto-detecting
*geometry* detected driver: dvips
*geometry* verbose mode - [ preamble ] result:
* driver: dvips
* paper: letterpaper
* layout: &lt;same size as paper&gt;
* layoutoffset:(h,v)=(0.0pt,0.0pt)
* modes: 
* h-part:(L,W,R)=(72.27pt, 469.75499pt, 72.27pt)
* v-part:(T,H,B)=(72.26999pt, 867.23999pt, -144.54pt)
* \paperwidth=614.295pt
* \paperheight=794.96999pt
* \textwidth=469.75499pt
* \textheight=867.23999pt
* \oddsidemargin=0.00002pt
* \evensidemargin=0.00002pt
* \topmargin=-37.0pt
* \headheight=12.0pt
* \headsep=25.0pt
* \topskip=12.0pt
* \footskip=30.0pt
* \marginparwidth=44.0pt
* \marginparsep=10.0pt
* \columnsep=10.0pt
* \skip\footins=10.8pt plus 4.0pt minus 2.0pt
* \hoffset=0.0pt
* \voffset=0.0pt
* \mag=1000
* \&#64;twocolumnfalse
* \&#64;twosidefalse
* \&#64;mparswitchfalse
* \&#64;reversemarginfalse
* (1in=72.27pt=25.4mm, 1cm=28.453pt)

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd)

LaTeX Warning: Command \textperiodcentered invalid in math mode on input line 3
0.


LaTeX Warning: Command \textperiodcentered invalid in math mode on input line 3
0.


! Package inputenc Error: Unicode character ｜ (U+FF5C)
(inputenc)                not set up for use with LaTeX.

See the inputenc package documentation for explanation.
Type  H &lt;return&gt;  for immediate help.
 ...                                              
                                                  
l.30 \fontsize{14}{17}\selectfont $a_t ~ \pi(·｜
                                                  s_t')$
[1] (./math.aux) )
(see the transcript file for additional information)
Output written on math.dvi (1 page, 448 bytes).
Transcript written on math.log.
</p>
</div>
.</p>
<p>In reinforcement learning，the policy gradient approach requires to learn a parametric representation of the policy (parameterized policy) by fitting a policy function with parameters. <img class="math" src="../_images/math/ed3f405ee761daf64d8e5baedd2d7bb26c33b1a8.svg" alt="\theta"/> is often used as the parameters. Another approach based on value functions does not necessarily require a policy gradient function. In the following sections we describe in a more detail the approach to learning polices in reinforcement learning.</p>
</div>
<div class="section" id="trajectory">
<h2>Trajectory<a class="headerlink" href="#trajectory" title="Permalink to this headline">¶</a></h2>
<p>In reinforcement learning, a sample learning sequence in a MDP is called <strong>trajectory</strong> <img class="math" src="../_images/math/a627d9386534e134e745389e9a86348befbfcc5b.svg" alt="(s_0, a_0, ..., s_n, a_n)"/>. Trajectory data includes a state transition function，namely, <img class="math" src="../_images/math/86afb8d63f3ddb889b490a00cf887e8fa498386e.svg" alt="s_{t+1} = f(s_t, a_t)"/> and a policy followed by the agents. Reinforcement learning contains two parts: how to use the policy to sample the trajectory data and how to use the trajectory data to update the learning target. The difference between the two components creates a difference in reinforcement learning methods.</p>
<p>As the trajectory also contains information about the dynamics of the model in the environment, it is possible to use the policy data to learn information about the environment as well, which can be used to help the learning of the intelligence. </p>
<p>Note that the transition function can be deterministic or stochastic. In a grid world, the trainsition function is determistic, i.e., an agent is going to go to a certain state given its current state and action. On the contrary, the state function is stochastic if an agent’s current state and action are given, but the agent may end up with more than one states with each probability samller than one. A stochastic state function is random in nature and cannot be determined completely by an agent.
This case can be easily illustrated in a simple MDP environment.</p>
</div>
<div class="section" id="return-and-reward">
<h2>Return and reward<a class="headerlink" href="#return-and-reward" title="Permalink to this headline">¶</a></h2>
<p><strong>Reward</strong> is learning signal assigned to an agent by its surrounding environment. When the enironment changes，the reward function also changes. The reward function is determined by the current state and the action taken by the agent，and can be written as <img class="math" src="../_images/math/8199da86c1ac580d647a2dd336db762cf93d9bc8.svg" alt="r_t = R(s_t, a_t)"/></p>
<p><strong>Cumulative Reward</strong> is the sum of the decaying returns from moment t onwards in a MDP.</p>
<p><img class="math" src="../_images/math/db1e5ba763d5c209e221840e9994f61e626ce421.svg" alt="G_t = R_{t+1}+\gamma * R_{t+2}+{\gamma}^2 * R_{t+3}+ ..."/></p>
<p><img class="math" src="../_images/math/1f7c32cd87f6900b6edb43745cc6eeb23eade7b4.svg" alt="\gamma"/> The discount factor reflects the ratio between the value of future rewards and that at the present moment. A value close to 0 indicates a tendency towards a ‘myopic’ assessment and a value close to 1 indicating a more forward-looking interest and confidence in the future. The introduction of the discount factor not only easy to express mathematically, but also avoids falling into an infinite loop and reduces the uncertainty of future benefits.</p>
<p>Other difficulties in dealing with reward functions may exist in different environments, such as sparse rewards where the environment does not give feedback in every state and only acquires rewards after a period of trajectory has elapsed. Therefore, the design and processing of reward functions in reinforcement learning are important directions that have a significant impact on the effectiveness of reinforcement learning.</p>
</div>
<div class="section" id="rl-optimization-problem">
<h2>RL optimization problem<a class="headerlink" href="#rl-optimization-problem" title="Permalink to this headline">¶</a></h2>
<p>In simple terms, the goal of a reinforcement learning problem is to find a policy that maximizes the expected total reward. Then, if we can calculate the return after each state by taking some action, we only need to take the action with the higher reward or the action that will lead to the states with the higher reward. Thus, the estimation of expected reward is also an optimisation direction for reinforcement learning. Another approach is to search directly over the action space. In either case, the ultimate optimisation goal is to maximise the reward.</p>
</div>
<div class="section" id="value-functions">
<h2>Value functions<a class="headerlink" href="#value-functions" title="Permalink to this headline">¶</a></h2>
<p><strong>State Value Function</strong> refers to a long-term expected reward by following a policy <img class="math" src="../_images/math/62f4dddfd372f3c595ec5e86cbad69f811c94f15.svg" alt="\pi"/> under a state <img class="math" src="../_images/math/6ca1d8558037c00a979b111265e9fd2de5069055.svg" alt="s"/> . The state value function is one of the criteria for evaluating a policy function</p>
<p><img class="math" src="../_images/math/dd91dea6d496a63ab93f5532af9f08803c266f90.svg" alt="V_{\pi}(s) = E_{\pi}[G_t|s_t=s]"/></p>
<p><strong>Action Value Function</strong> refers to a long-term expected reward by following a policy <img class="math" src="../_images/math/62f4dddfd372f3c595ec5e86cbad69f811c94f15.svg" alt="\pi"/> under a state <img class="math" src="../_images/math/6ca1d8558037c00a979b111265e9fd2de5069055.svg" alt="s"/> ,  and an action <img class="math" src="../_images/math/becb5d56210daf0b2a97f9aa1207dc03838b9fbd.svg" alt="a"/></p>
<p><img class="math" src="../_images/math/d7db051b9fc5ee7e6c44be682d91937ba7dc3bb1.svg" alt="Q_{\pi}(s, a) = E_{\pi}[G_t|s_t=s, a_t=a]"/></p>
<p>The relationship between the state-valued function and the action-valued function：</p>
<p><img class="math" src="../_images/math/ee620af699c1d58769902b2fd3b48d3adcf23fb8.svg" alt="V_{\pi}(s) = \sum \pi(a|s)Q_{\pi}(s,a)"/></p>
<p>We can further obtain the relationship between the optimal state value function and the optimal behavioural value function as follows.</p>
<p><img class="math" src="../_images/math/e5e7b5cdeaffea08d414e8c956c39e167e4cc353.svg" alt="V*(s)=max_a Q*(s, a)"/></p>
<p><strong>Bellman Equations</strong>，The Bellman’s equation is the basis of reinforcement learning. The Bellman equation represents the value of the current state in relation to the value of the next state, and the current reward.
We can express the state value function and the action value function as:</p>
<p><img class="math" src="../_images/math/1021785052deb07e751cbce77345ddf290cef24f.svg" alt="V_{\pi}(s) = E_{\pi}[R_{t+1}+\gamma * v_{\pi}(s_{t+1})|s_t=s]"/></p>
<p><img class="math" src="../_images/math/1381069829ee4373310cf11bb9b4411c596f1a62.svg" alt="Q_{\pi}(s, a) = E_{\pi}[R_{t+1}+\gamma * Q(s_{t+1},a_{t+1})|s_t=s, a_t=a]"/></p>
<p><strong>Bellman Optimality Equations</strong>，</p>
<p><img class="math" src="../_images/math/ed47d6e3f4a813f61aea8584bc7c46d383bb70d7.svg" alt="V*(s)=E[R_{t+1} + \gamma * max_{\pi}V(s_{t+1})|s_t=s]"/></p>
<p><img class="math" src="../_images/math/eae858b8b5b3346e312d04db5b89db6fc53c49af.svg" alt="Q*(s, a) = E_{\pi}[R_{t+1}+\gamma * max_{a'}Q(s_{t+1},a')|s_t=s, a_t=a]"/></p>
<p>Value based reinforcement learning approach includes two steps：policy evalution and policy improvement. Reinforcement learning first estimate the value function based on the policy，then, improves the policy according to the value function. When the value function reaches the optima, the policy is considered as the optimal policy. This optimal policy is a greedy policy.</p>
<p>For systems where the model is known, the value function can be obtained using DPs; For systems where the model is unknown, it can be obtained using MC or TD.</p>
<p>For a grid reinforcement learning environment，the estimation of the value function is obtained by iteratively updating the table of value functions. In many cases，say, state space and action space are not discrete，the value function cannot be represented by a table. In this situation, we need to take advantage of function approximation to approximate the value function.</p>
</div>
<div class="section" id="policy-gradients">
<h2>Policy Gradients<a class="headerlink" href="#policy-gradients" title="Permalink to this headline">¶</a></h2>
<p>In some situations，a stocatic policy is better than a deterministic policy. As a result, value-based reinforcement learning cannot learn such policy and a policy-based approach to reinforcement learning is therefore proposed.</p>
<p>Unlike value-based reinforcement learning, policy-based reinforcement learning parameterise the policy and represent it by using linear or non-linear functions to find the optimal parameters that maximise the expectation of the cumulative reward, the goal of reinforcement learning.</p>
<p>In the value-based approach, we iteratively compute the value function and then improve the policy based on the value function, whereas in the policy search approach, we directly compute the policy iteration using <strong>policy gradient</strong>, i.e. we compute the policy gradient on the action, and iteratively update the policy parameter values along the gradient until the expectation of cumulative return is maximised, at which point the policy corresponding to the parameter is the optimal policy.</p>
<p>Comparing to the value-based approach, the policy gradient reinforcement learning tends to converge to a local minimum, which is not sufficient when evaluating an individual policy and has a large variance.</p>
<p>For a more detailed understanding of the policy based approach, please refer to the specific algorithms in our documentation：　<a class="reference external" href="../hands_on/index.html">Hans On
RL</a></p>
</div>
<div class="section" id="actor-critic">
<h2>Actor Critic<a class="headerlink" href="#actor-critic" title="Permalink to this headline">¶</a></h2>
<p><strong>Critic</strong>, parametrized behavioural value function; performs the value evaluation of the policy.</p>
<p><strong>Actor</strong>, parametrized policy function, performs an update of the policy function parameters using the policy gradient according to the value obtained in the Critic part.</p>
<p>In summary, Actor Critic is an approach that learns both the value function and the policy function, combining the advantages of both of these approaches. Various algorithms based on this framework can adapt to problems in different action and state spaces as well as to find optimal policies in different policy spaces.</p>
<p>More Actor Critic algorithms such as A2C, DDPG, TD3, etc. are explained in our documentation.</p>
</div>
<div class="section" id="model-based-rl">
<h2>Model-based RL<a class="headerlink" href="#model-based-rl" title="Permalink to this headline">¶</a></h2>
<p>Of the above model-free approaches, the value-based approach learns the value function (MC or TD) before updating the policy, while the policy-based approach updates the policy directly. The model-based approach focuses on the environment dynamics, where a model of the environment is learned through sampling, and then the value function/ policy is optimised based on the learned environment model.</p>
<p>Once the modeling of the environment has been completed, there are also two paths in the model-based approach: one is to generate some simulation trajectories from the learned model and estimate the value function from the simulation trajectories to optimise the strategy; the other is to optimise the policy directly from the learned model, which is the route the model-based approach is usually taking. Learning a model of the environment first can help us to solve the problem of sample efficiency in reinforcement learning methods.</p>
<p>The definition of a model can be expressed mathematically as a tuple of state transfer distributions and reward functions.</p>
<p><img class="math" src="../_images/math/8105c11bf07d97e49c01d505c6feb07b2893d3fa.svg" alt="M=(P,R), s_{t+1}~P(s_{t+1}|s_t, a_t), r_{t+1}~R(r_{t+1}|s_t, a_t)"/></p>
<p>The learning of a model can be extended to different algorithms depending on the model construction.</p>
<p>Model-based policy optimisation: A classical approach is to first sample a large amount of data by some strategy, then learn a model to minimise the error, apply the learned model to planning to obtain new data, and repeat the above steps. It is by doing planning on top of the learned model that model-based improves the efficiency of the entire iteration of the reinforcement learning algorithm.</p>
</div>
<div class="section" id="q-a">
<h2>Q&amp;A<a class="headerlink" href="#q-a" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Q1: What are model-based and model-free methods，what are the differences？Which category should MC、TD、DP, etc. belong to?</dt><dd><ul class="simple">
<li><p>Answer：
model based algorithm means that the algorithm learns the state transition process of the environment and models the environment, whereas a model free algorithm does not require the environment to be modelled.
Monte Carlo and TD algorithms are model-free because they do not require the algorithm to model a specific environment.
Dynamic programming, on the other hand, is model-based, as the use of dynamic programming requires a complete model of the environment.</p></li>
</ul>
</dd>
<dt>Q2: What do we mean by value-based， policy-based and collector-critic？ which algorithms can be classified as value-based，policy-based or actor-critic？what advantages do they have？what about the disadvantages？</dt><dd><ul class="simple">
<li><p>Answer：Value-based is to learn how to do critic (judging the value of an input state). Policy-based is to learn how to do actor (judging what action should be taken in an input state), and actor-critic is to learn decide critic while training the actor network.
The relationship of these three classes can be well explained by the following diagram.</p></li>
</ul>
</dd>
</dl>
<a class="reference internal image-reference" href="../_images/actor-critic.jpg"><img alt="../_images/actor-critic.jpg" src="../_images/actor-critic.jpg" style="width: 527.1px; height: 228.6px;" /></a>
<dl class="simple">
<dt>Q3: What are on-policy and off-policy？</dt><dd><ul class="simple">
<li><p>Answer：The on-policy algorithms are trained using the current policy. The policy used to generate sampled data is the same as the policy to be evaluated and improved.
Off-policy algorithm, on the other hand, can be trained using the policy from the previous process, and the policy used to generate the sampled data is different from the policy to be evaluated and improved, i.e., the data generated is “off” the trajectory of the decision series determined by the policy to be optimised.
On-policy and off-policy simply means how training is done, and sometimes an algorithm may even have different ways of  implementation of on-policy and off-policy.</p></li>
</ul>
</dd>
<dt>Q4: What are online training and offline training？ How do we implement ffline training？</dt><dd><ul class="simple">
<li><p>Answer： Offline training means the training uses fixed datasets as input  instead of using a collector to interact with the environment. For example, behavioural cloning is a classic offline training algorithm. We usually input batch data in a fixed dataset, hence, offline RL is also called batch RL.</p></li>
</ul>
</dd>
<dt>Q5: What are expolration and expolitation？What methods do we use to balance expolration and expolitation？</dt><dd><ul class="simple">
<li><p>Answer：Exploration is when an agent in RL is constantly exploring different states of the environment, while exploitation is when the agent selects the most rewarding action possible for the current state.
There are many ways to balance exploration and exploitation. There are also different ways of implementations in different algorithms. With respect to sampling in discrete action spaces, one can follow a probability distribution or select randomly. With respect to sampling in continuous action spaces, one can follow a coutinuous distribution or add NOISE.</p></li>
</ul>
</dd>
<dt>Q6: Why do we use replay buffer？ why do we neew experience replay？</dt><dd><ul class="simple">
<li><p>AnswerBy using the replay buffer, we can store the experiences in the buffer and sample the experiences in the buffer during subsequent training. Experience replay is a technique that saves samples from the system’s exploration of the environment and then samples them to update the model parameters.</p></li>
</ul>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../hands_on/index.html" class="btn btn-neutral float-right" title="RL Algorithm Cheat Sheet" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Introduction to RL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, OpenDILab Contributors

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>